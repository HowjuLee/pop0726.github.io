<html>
<head>
<title>人工智能原理</title>
<meta http-equiv="Content-Type" content="text/html; charset="utf-8"">
<link rel="stylesheet" href="../../../css/text.css" type="text/css">
</head>

<body bgcolor="#FFFFFF" text="#000000">
<table width="97%" border="0" cellspacing="0" cellpadding="0" align="right">
  <tr>
    <td><span class="text"> 
      <table width="100%" border="0" cellspacing="3" cellpadding="2">
        <tr> 
          <td> 
            <table width="50%" border="0" cellspacing="0" cellpadding="0" height="18" align="right">
              <tr> 
                <td class="pt10" background="../../../images/pic/bg0401.gif"> 
                  <div align="center" class="chap">第八章 自然语言理解的任务和发展简史</div>
                </td>
              </tr>
            </table>
          </td>
        </tr>
      </table>
      </span><br>
      　　<span class="section">8.4 以走向实用化和工程化为特征的近期历史</span><span class="text"><br>
      <br>
      　　有些研究人员把进入八十年代以后的自然语言理解系统称为基于知识的新一代理解系统，并且认为这是今后一个时期发展的主要方向。我们觉得这样一种认识未必恰当；原因是：<br>
      　　(1)早期和中期的系统除了要依靠语言学的知识以外，也或多或少地求助于主题领域的世界知识。正如STUDENT系统需要有一定的代数和单位换算的知识、ELIZA需要有某些心理治疗的知识一样，LUNAR系统需要有岩石标本和化学分析方面的知识。SHRDLU需要有积木世界的知识。MARGIE系统为进行推理或释义也需要有关的背景知识。所以说引入世界知识和某些推理机制，无论是在早期还是在中期的系统中都是早已有之，不足以作�&quot;新一代&quot;系统的特征。 
      <br>
      　　(2)人工智能系统，不论是专家系统；决策支持系统、机器人视觉系统、机器人路径规划系统、计算机博弈系统，还是自然语言理解系统，都离不开指定领域的知识和某种推理机制，所以它们统通可以称为是基于知识的计算机系统。可见，&quot;基于知识&quot;这个术语失于笼统，用它来作为划代的依据也就难于令人信服。<br>
      　　我们认为进入80年代以来自然语言理解系统的最大特点就是实用化和工程化。其重要标志就是一批商品化的自然语言人-机接口和机器翻译系统出现在国际市场上。著名的有美国人工智能公司(AIC)生产的英语人-机接口系统Intellect，美国弗雷公司生产的Themis人-机接口，美国加里福尼亚工学院研制的ASK接口；欧洲共同体在美国乔治敦大学开发的机译系统SYSTRAN的基础上成功地进行了英、法、德、西、意、葡等多语对的机器翻译，加拿大蒙特利尔大学开发的服务于天气预报领域的英法机译系统TAUM-METEO，日本富士通公司开发的ATLAS英日、日英机译系统，日本日立公司开发的HICATS英日、日英机译系统等等。国内&quot;七五&quot;期间由中国软件总公司开发的商品化英汉机译系统&quot;译星&quot;(TRANSTAR)，也是这方面的一个范例。<br>
      　　1990年8月，在赫尔辛基召开的第13届国际计算语言学大会上，大会组织者首次提出了处理大规模真实文本的战略目标，并在会前组织了&quot;大型语料库在建造自然语言系统中的作用&quot;、&quot;词典知识的获取与表示&quot;和&quot;电子词典&quot;等专题讲座，预告了语言信息处理的一个新的历史阶段即将到来。<br>
      　　从自然语言理解的发展历史来看，30年来其主流技术一直是句法-语义分析，它的许多思想来自人工智能，这就决定了它的主要方法是基于规则的。但是实践已经证明，在当今计算技术的限度内，要想把理解自然语言所需的知识(包括语言学知识和语言学以外的知识)都用规则形式表达出来，是不可能的。这既是由于这种知识�&quot;数量&quot;浩瀚无际，又是由于它们在&quot;质&quot;的方面高度的不确定性和模糊性。这种情况也说明了为什么多年来这类基于规则的自然语言系统只能在极其受限的某些子语言中获得有限的成功。<br>
      　　处理大规模真实文本的目标同只处理受限语言的目标大相径庭。目标的不同必然导致观念的变化。为实现大规模真实文本的处理，自然语言处理的理论、方法和工具都需要有一个新的发展。<br>
      　　这里值得注意的是在最近十年中崛起的语料库语言学(Corpus Linguistics)。首先它顺应大规模真实文本处理的需求，提出了以计算机语料库为基础的语言学研究及自然语言处理的新思想。这个学派坚持认为语言学知识的真正源泉是大规模活生生的语料，计算语言学工作者的任务是使计算机能自动或半自动地从大规模语料库中获取理解语言所需的各种知识,他们必须客观地而不是主观地对库存的语言事实作出描述。 
      <br>
      　　要想让计算机来自动获取如此大量的知识，目前的一个可行方案是采用统计模型。例如IBM沃森研究中心提出n元语法(n-gram)。n元语法研究一个符号串中n个相邻符号同现的概率，这是一种(n-1)阶的马尔可夫模型。目前广泛应用于语音识别、字符识别(OCR)、自动分词、词类标注和句法分析等领域的是n元语法中最简单的二元语法(bigram)和三元语法(trigram)。<br>
      　　举例来说，70年代Greene和Rubin曾设计过一个TAGGIT系统，用来对布朗语料库的一百万词英语语料进行词类的自动标注。他们采用的词类标记(tags)共有86种，系统使用了3300条上下文有关规则，标注的成功率达77％。80年代英国兰开斯特大学Leech领导的UCREL研究小组，利用已带有词类标记的布朗语料库，经过统计分析获得一个反映任意两个相邻标记同现频率的&quot;概率转移矩阵&quot;。他们设计的CLAWS系统依据这种统计信息而不是规则，对LOB语料库的一百万词语料进行词类的自动标注，成功率跃升到96％。目前UCREL小组和国际上许多研究团体正借助于这种统计模型对大规模语料进行句法分析的实验。<br>
      　　CLAWS系统的经验所带给我们的启示是深刻的。许多研究人员相信，基于语料库的统计模型不仅能胜任词类的自动标注任务，而且也能够应用到句法和语义等更高层次的分析上来。这种方法有希望在工程上、在宽广的语言覆盖面上解决大规模真实文本处理这一极其艰巨的课题，至少也能对基于规则的自然语言处理系统提供一种强有力的补充机制。<br>
      　　由于语料库语言学诞生不久，基于语料库的自然语言理解方法还很不成熟，我们无法在本书的篇幅中给予系统的介绍，但是这确实是值得引起人们充分重视的一个研究方向。 
      <br>
      </span> </td>
  </tr>
</table>
</body>
</html>
